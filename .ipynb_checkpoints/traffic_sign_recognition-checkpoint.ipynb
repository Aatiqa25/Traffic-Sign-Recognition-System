{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traffic Sign Recognition System\n",
        "\n",
        "## 1. Data Loading and Exploration\n",
        "Loading the GTSRB dataset and examining its structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'Data'\n",
        "train_csv = pd.read_csv(os.path.join(data_dir, 'Train.csv'))\n",
        "meta_csv = pd.read_csv(os.path.join(data_dir, 'Meta.csv'))\n",
        "\n",
        "print(f\"Training data shape: {train_csv.shape}\")\n",
        "print(f\"Number of classes: {len(train_csv['ClassId'].unique())}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(train_csv['ClassId'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Visualization\n",
        "Visualizing sample traffic signs from different classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "classes = sorted(train_csv['ClassId'].unique())\n",
        "for i, class_id in enumerate(classes[:20]):\n",
        "    class_data = train_csv[train_csv['ClassId'] == class_id]\n",
        "    sample_path = os.path.join(data_dir, class_data.iloc[0]['Path'])\n",
        "    image = cv2.imread(sample_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    plt.subplot(4, 5, i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f'Class {class_id}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "Loading and preprocessing images with resizing and normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = 32\n",
        "NUM_CLASSES = 43\n",
        "\n",
        "def load_and_preprocess_data(csv_file, data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    for idx, row in csv_file.iterrows():\n",
        "        img_path = os.path.join(data_dir, row['Path'])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        image = image.astype('float32') / 255.0\n",
        "        \n",
        "        images.append(image)\n",
        "        labels.append(row['ClassId'])\n",
        "    \n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "print(\"Loading and preprocessing training data...\")\n",
        "X_train, y_train = load_and_preprocess_data(train_csv, data_dir)\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
        "\n",
        "y_train_categorical = to_categorical(y_train, NUM_CLASSES)\n",
        "y_val_categorical = to_categorical(y_val, NUM_CLASSES)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train_categorical.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val_categorical.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. CNN Model Architecture\n",
        "Building a Convolutional Neural Network for traffic sign classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.25),\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Augmentation\n",
        "Applying data augmentation to improve model generalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Training\n",
        "Training the CNN model with callbacks for optimal performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train_categorical, batch_size=BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, y_val_categorical),\n",
        "    callbacks=[reduce_lr, early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Visualization\n",
        "Plotting training and validation accuracy and loss curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Evaluation\n",
        "Evaluating model performance with accuracy and confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_val)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred_classes)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred_classes)\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred_classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Saving\n",
        "Saving the trained model in both H5 and Pickle formats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('traffic_sign_model.h5')\n",
        "print(\"Model saved as traffic_sign_model.h5\")\n",
        "\n",
        "with open('traffic_sign_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "print(\"Model saved as traffic_sign_model.pkl\")\n",
        "\n",
        "class_names = {i: f'Class_{i}' for i in range(NUM_CLASSES)}\n",
        "with open('class_names.pkl', 'wb') as f:\n",
        "    pickle.dump(class_names, f)\n",
        "print(\"Class names saved as class_names.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
